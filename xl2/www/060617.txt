<!--About bugs and integers-->

Joshua Bloch, a very smart guy I had the privilege to meet once, wrote
<A href="http://googleresearch.blogspot.com/2006/06/extra-extra-read-all-about-it-nearly.html">this interesting piece</A>
about how a subtle bug crept in the binary search he implemented
for Sun. You should probably read it first...

It's interesting in the context of XL, because it illustrates very
well the kind of bugs that arise at the boundary between concepts and
code. The bug does not exist in the concept space, a guy named Jon
Bentley had proven it. The mathematical algorithm itself is correct, it's
only the implementation with a limited size of integers that fails.

Another interesting piece is a
a <A href="http://patricklogan.blogspot.com/2006/06/integers.html">reply
from Patrick Logan</A>. I am tempted to grant him the <em>missed
point award</em>. Why?

<b>Getting integers right?</b>
Patrick Logan's reasoning starts with: <em>"Psst... look at the code. You
*did* get binary search right.  The problem is you got integers
wrong"</em>. And there, he has a point, doesn't he? The {tt "int"}
type in Java is not, despite its name, a very close approximation of
concept-space integers.

But then, Patrick Logan completely misses it by suggesting that
the solution is to have language-mandated infinite-precision
integers. And he goes on to "prove" it with an implementation of
factorial that can compute the factorial of 1000.

<b>Artificial complexity</b>:
Well, let me start by saying that I find his implementation quite
obscure. Not just because of Lisp cultists' insistance on using prefix polish
notation (Who in the real world writes {tt "(&lt;&nbsp;n&nbsp;2)"} instead of
{tt "n&lt;2"}?), but also because it's full of the kind of clever
micro-hacks that pepper too many functional programs (why are there
two tests for the condition n&lt;2?).

<b>Problem <em>not</em> solved</b>:
But that's really not my main concern with this reply. Rather, it's
the belief expressed in it that this solves anything. Sure, it is
possible to simulate better integers, and it might be worth it if they
were perfect. But they are not.

Simply consider Patrick Logan's "proof": granted, your code can
compute <em>factorial (1000)</em>, but it still cannot compute
<em>factorial(factorial(factorial(1000)))</em>, because there are
probably not enough atoms in the universe to build the memory required
to hold that value. So, Patrick, the integer representation you advertise
is <em>also</em> wrong, just at a different scale than Java's.

In many cases, including Joshua Bloch's bug, they are not even a marked
improvement over fixed-size integers, as I'll prove below. And then,
you now have an implementation of integer addition that <em>consumes
memory</em>. Talk about side effects!

<b>Knowing when concepts and code don't match</b>:
It is better to know that you got things wrong, as I advocate with
concept programming, than to believe that you have everything right when
it's not the case. And there is value in knowning just exactly where you
are wrong, or the limits of the particular machine approximation you
run.

So what limits are there in the case of the Java binary search code?
Patrick Logan suggested adding a new abstraction: does it help with
the binary search problem? Actually, it does not. One can easily prove that
the largest integer you will need to index your array is the number of
addressable units in the machine. For a machine with a 64-bit address
space, you will <em>never</em> need more than 64-bit for a correct
implementation of binary search <em>for any array that might fit in
the machine's memory</em>.

That is the right answer to the original bug. "Big" integers might be the
right answer to some other problem, but definitely not the one Joshua Bloch
exposed. That reasoning also shows that the solution he gave is
only partial. It extends the correctness domain of his implementation
from 30 to 31 bits, but on a 64-bit machine, it is <em>still</em>
possible to have large enough arrays where the implementation will
fail, simply because it uses a 32-bit index into an array that can
contain more than 4 billion elements.

<b>A dose of reality</b>:
The key reason most programming languages (including XL) feature
support for pseudo-integer types like what we improperly call 32-bit
or 64-bit integers is that these are machine-supported types. Machine
registers holds values like this easily, load-store operations to
memory operate on such values, and primitive machine operations take
such pseudo-numbers as operands.

A concept programming language must support this kind of operations
because they are the closest thing to "reality" in the computer
world. There will never be a real "integer" in the machine, but
chances are that there a true implementation of some fixed size
arithmetic.

I'd go as far as saying that a 32-bit integer is probably better
suited to formal reasoning than a "big" integer, because it is much
easier to define when the addition of two 32-bit values overflows than
to define when the addition of two memory-constrained integers will
run into some machine resource limit.
